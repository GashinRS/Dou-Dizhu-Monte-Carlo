{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potato\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found log directory: experiments/dmc_result\\doudizhu\n",
      "Saving arguments to experiments/dmc_result\\doudizhu/meta.json\n",
      "Path to meta file already exists. Not overriding meta.\n",
      "Saving messages to experiments/dmc_result\\doudizhu/out.log\n",
      "Path to message file already exists. New data will be appended.\n",
      "Saving logs data to experiments/dmc_result\\doudizhu/logs.csv\n",
      "Saving logs' fields to experiments/dmc_result\\doudizhu/fields.csv\n",
      "Path to log file already exists. New data will be appended.\n",
      "[INFO:16912 trainer:265 2022-11-23 17:58:50,523] Resuming preempted job, current stats:\n",
      "{'mean_episode_return_0': 0.4944831430912018, 'loss_0': 0.19426479935646057, 'mean_episode_return_1': 0.5110607743263245, 'loss_1': 0.23292842507362366, 'mean_episode_return_2': 0.5118946433067322, 'loss_2': 0.22866547107696533}\n",
      "[INFO:16912 trainer:335 2022-11-23 17:59:08,399] Saving checkpoint to experiments/dmc_result/doudizhu/model.tar\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:08,804] After 1052800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 0.19426479935646057,\n",
      " 'loss_1': 0.23292842507362366,\n",
      " 'loss_2': 0.22866547107696533,\n",
      " 'mean_episode_return_0': 0.4944831430912018,\n",
      " 'mean_episode_return_1': 0.5110607743263245,\n",
      " 'mean_episode_return_2': 0.5118946433067322}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:13,814] After 1062400 frames: @ 1917.1 fps Stats:\n",
      "{'loss_0': 0.17833012342453003,\n",
      " 'loss_1': 0.20243749022483826,\n",
      " 'loss_2': 0.2120947390794754,\n",
      " 'mean_episode_return_0': 0.552742600440979,\n",
      " 'mean_episode_return_1': 0.45748987793922424,\n",
      " 'mean_episode_return_2': 0.4584980309009552}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:18,827] After 1072000 frames: @ 1915.4 fps Stats:\n",
      "{'loss_0': 0.1735878586769104,\n",
      " 'loss_1': 0.19802279770374298,\n",
      " 'loss_2': 0.20993146300315857,\n",
      " 'mean_episode_return_0': 0.5581509470939636,\n",
      " 'mean_episode_return_1': 0.44215959310531616,\n",
      " 'mean_episode_return_2': 0.4412490129470825}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:23,852] After 1078400 frames: @ 1273.9 fps Stats:\n",
      "{'loss_0': 0.17074768245220184,\n",
      " 'loss_1': 0.20600084960460663,\n",
      " 'loss_2': 0.20993146300315857,\n",
      " 'mean_episode_return_0': 0.5575957298278809,\n",
      " 'mean_episode_return_1': 0.4355432987213135,\n",
      " 'mean_episode_return_2': 0.4412490129470825}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:28,869] After 1081600 frames: @ 638.1 fps Stats:\n",
      "{'loss_0': 0.17074768245220184,\n",
      " 'loss_1': 0.20600084960460663,\n",
      " 'loss_2': 0.21422472596168518,\n",
      " 'mean_episode_return_0': 0.5575957298278809,\n",
      " 'mean_episode_return_1': 0.4355432987213135,\n",
      " 'mean_episode_return_2': 0.42933663725852966}\n"
     ]
    }
   ],
   "source": [
    "from rlcard.envs import make\n",
    "from rlcard.agents.dmc_agent.trainer import DMCTrainer\n",
    "\n",
    "print(\"potato\")\n",
    "env = make(\"doudizhu\")\n",
    "# Initialize the DMC trainer\n",
    "trainer = DMCTrainer(\n",
    "    env,\n",
    "    cuda='',\n",
    "    load_model=True,\n",
    "    xpid=\"doudizhu\",\n",
    "    savedir='experiments/dmc_result',\n",
    "    save_interval=10,\n",
    "    num_actor_devices=1,\n",
    "    num_actors=5,\n",
    "    training_device=\"0\",\n",
    ")\n",
    "\n",
    "# Train DMC Agents\n",
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the GPU\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  55\n",
      "  reward       |  0.0195\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "  timestep     |  117037\n",
      "  reward       |  0.0185\n",
      "----------------------------------------\n",
      "\n",
      "Logs saved in experiments/doudizhu_dqn/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 83\u001B[0m\n\u001B[0;32m     81\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(agent, save_path)\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel saved in\u001B[39m\u001B[38;5;124m'\u001B[39m, save_path)\n\u001B[1;32m---> 83\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [3], line 67\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(eval)\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;66;03m# Evaluate the performance. Play with random agents.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28meval\u001B[39m:\n\u001B[0;32m     65\u001B[0m         logger\u001B[38;5;241m.\u001B[39mlog_performance(\n\u001B[0;32m     66\u001B[0m             env\u001B[38;5;241m.\u001B[39mtimestep,\n\u001B[1;32m---> 67\u001B[0m             \u001B[43mtournament\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m                \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m                \u001B[49m\u001B[43mnum_eval_games\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     71\u001B[0m         )\n\u001B[0;32m     73\u001B[0m \u001B[38;5;66;03m# Get the paths\u001B[39;00m\n\u001B[0;32m     74\u001B[0m csv_path, fig_path \u001B[38;5;241m=\u001B[39m logger\u001B[38;5;241m.\u001B[39mcsv_path, logger\u001B[38;5;241m.\u001B[39mfig_path\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\utils\\utils.py:210\u001B[0m, in \u001B[0;36mtournament\u001B[1;34m(env, num)\u001B[0m\n\u001B[0;32m    208\u001B[0m counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m counter \u001B[38;5;241m<\u001B[39m num:\n\u001B[1;32m--> 210\u001B[0m     _, _payoffs \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mis_training\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(_payoffs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m _p \u001B[38;5;129;01min\u001B[39;00m _payoffs:\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\envs\\env.py:144\u001B[0m, in \u001B[0;36mEnv.run\u001B[1;34m(self, is_training)\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_over():\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;66;03m# Agent plays\u001B[39;00m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_training:\n\u001B[1;32m--> 144\u001B[0m         action, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magents\u001B[49m\u001B[43m[\u001B[49m\u001B[43mplayer_id\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    146\u001B[0m         action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magents[player_id]\u001B[38;5;241m.\u001B[39mstep(state)\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:163\u001B[0m, in \u001B[0;36mDQNAgent.eval_step\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21meval_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;124;03m''' Predict the action for evaluation purpose.\u001B[39;00m\n\u001B[0;32m    155\u001B[0m \n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m        info (dict): A dictionary containing information\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m--> 163\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    164\u001B[0m     best_action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(q_values)\n\u001B[0;32m    166\u001B[0m     info \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:181\u001B[0m, in \u001B[0;36mDQNAgent.predict\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;124;03m''' Predict the masked Q-values\u001B[39;00m\n\u001B[0;32m    173\u001B[0m \n\u001B[0;32m    174\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m        q_values (numpy.array): a 1-d array where each entry represents a Q value\u001B[39;00m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[1;32m--> 181\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_nograd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_dims\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mobs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    182\u001B[0m     masked_q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_actions, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m)\n\u001B[0;32m    183\u001B[0m     legal_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlegal_actions\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:297\u001B[0m, in \u001B[0;36mEstimator.predict_nograd\u001B[1;34m(self, s)\u001B[0m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    296\u001B[0m     s \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(s)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 297\u001B[0m     q_as \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m q_as\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "import os\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "def train(eval):\n",
    "\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "\n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(42)\n",
    "\n",
    "    num_episodes = 500000\n",
    "    num_eval_games = 2000\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(\n",
    "        \"doudizhu\",\n",
    "        config={\n",
    "            'seed': 42,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    from rlcard.agents import DQNAgent\n",
    "#     agent = DQNAgent(\n",
    "#         num_actions=env.num_actions,\n",
    "#         state_shape=env.state_shape[0],\n",
    "#         mlp_layers=[64,64],\n",
    "#         device=device,\n",
    "#     )\n",
    "\n",
    "    agent = torch.load(\"experiments/doudizhu_dqn/model8000.pth\")\n",
    "\n",
    "    agents = [agent]\n",
    "    for _ in range(1, 3):\n",
    "        agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    # Start training\n",
    "    with Logger(\"experiments/doudizhu_dqn/\") as logger:\n",
    "        for episode in range(8001, num_episodes):\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            # if eval:\n",
    "            #     logger.log_performance(\n",
    "            #         env.timestep,\n",
    "            #         tournament(\n",
    "            #             env,\n",
    "            #             num_eval_games,\n",
    "            #         )[0]\n",
    "            #     )\n",
    "\n",
    "            if episode % 1000 == 0 and episode:\n",
    "                save_path = os.path.join(\"experiments/doudizhu_dqn/\", f'model{episode}.pth')\n",
    "                torch.save(agent, save_path)\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, \"dqn\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(\"experiments/doudizhu_dqn/\", 'model.pth')\n",
    "    torch.save(agent, save_path)\n",
    "    print('Model saved in', save_path)\n",
    "train(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
