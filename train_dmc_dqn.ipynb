{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potato\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found log directory: experiments/dmc_result\\doudizhu\n",
      "Saving arguments to experiments/dmc_result\\doudizhu/meta.json\n",
      "Path to meta file already exists. Not overriding meta.\n",
      "Saving messages to experiments/dmc_result\\doudizhu/out.log\n",
      "Path to message file already exists. New data will be appended.\n",
      "Saving logs data to experiments/dmc_result\\doudizhu/logs.csv\n",
      "Saving logs' fields to experiments/dmc_result\\doudizhu/fields.csv\n",
      "Path to log file already exists. New data will be appended.\n",
      "[INFO:16912 trainer:265 2022-11-23 17:58:50,523] Resuming preempted job, current stats:\n",
      "{'mean_episode_return_0': 0.4944831430912018, 'loss_0': 0.19426479935646057, 'mean_episode_return_1': 0.5110607743263245, 'loss_1': 0.23292842507362366, 'mean_episode_return_2': 0.5118946433067322, 'loss_2': 0.22866547107696533}\n",
      "[INFO:16912 trainer:335 2022-11-23 17:59:08,399] Saving checkpoint to experiments/dmc_result/doudizhu/model.tar\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:08,804] After 1052800 frames: @ 0.0 fps Stats:\n",
      "{'loss_0': 0.19426479935646057,\n",
      " 'loss_1': 0.23292842507362366,\n",
      " 'loss_2': 0.22866547107696533,\n",
      " 'mean_episode_return_0': 0.4944831430912018,\n",
      " 'mean_episode_return_1': 0.5110607743263245,\n",
      " 'mean_episode_return_2': 0.5118946433067322}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:13,814] After 1062400 frames: @ 1917.1 fps Stats:\n",
      "{'loss_0': 0.17833012342453003,\n",
      " 'loss_1': 0.20243749022483826,\n",
      " 'loss_2': 0.2120947390794754,\n",
      " 'mean_episode_return_0': 0.552742600440979,\n",
      " 'mean_episode_return_1': 0.45748987793922424,\n",
      " 'mean_episode_return_2': 0.4584980309009552}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:18,827] After 1072000 frames: @ 1915.4 fps Stats:\n",
      "{'loss_0': 0.1735878586769104,\n",
      " 'loss_1': 0.19802279770374298,\n",
      " 'loss_2': 0.20993146300315857,\n",
      " 'mean_episode_return_0': 0.5581509470939636,\n",
      " 'mean_episode_return_1': 0.44215959310531616,\n",
      " 'mean_episode_return_2': 0.4412490129470825}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:23,852] After 1078400 frames: @ 1273.9 fps Stats:\n",
      "{'loss_0': 0.17074768245220184,\n",
      " 'loss_1': 0.20600084960460663,\n",
      " 'loss_2': 0.20993146300315857,\n",
      " 'mean_episode_return_0': 0.5575957298278809,\n",
      " 'mean_episode_return_1': 0.4355432987213135,\n",
      " 'mean_episode_return_2': 0.4412490129470825}\n",
      "[INFO:16912 trainer:367 2022-11-23 17:59:28,869] After 1081600 frames: @ 638.1 fps Stats:\n",
      "{'loss_0': 0.17074768245220184,\n",
      " 'loss_1': 0.20600084960460663,\n",
      " 'loss_2': 0.21422472596168518,\n",
      " 'mean_episode_return_0': 0.5575957298278809,\n",
      " 'mean_episode_return_1': 0.4355432987213135,\n",
      " 'mean_episode_return_2': 0.42933663725852966}\n"
     ]
    }
   ],
   "source": [
    "from rlcard.envs import make\n",
    "from rlcard.agents.dmc_agent.trainer import DMCTrainer\n",
    "\n",
    "print(\"potato\")\n",
    "env = make(\"doudizhu\")\n",
    "# Initialize the DMC trainer\n",
    "trainer = DMCTrainer(\n",
    "    env,\n",
    "    cuda='',\n",
    "    load_model=True,\n",
    "    xpid=\"doudizhu\",\n",
    "    savedir='experiments/dmc_result',\n",
    "    save_interval=10,\n",
    "    num_actor_devices=1,\n",
    "    num_actors=5,\n",
    "    training_device=\"0\",\n",
    ")\n",
    "\n",
    "# Train DMC Agents\n",
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running on the GPU\n",
      "\n",
      "Logs saved in experiments/doudizhu_dqn/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'capturable'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [12], line 111\u001B[0m\n\u001B[0;32m    109\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(agent_landlord, save_path)\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel saved in\u001B[39m\u001B[38;5;124m'\u001B[39m, save_path)\n\u001B[1;32m--> 111\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [12], line 79\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(eval)\u001B[0m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;66;03m# Feed transitions into agent memory, and train the agent\u001B[39;00m\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;66;03m# Here, we assume that DQN always plays the first position\u001B[39;00m\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;66;03m# and the other players play randomly (if any)\u001B[39;00m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ts \u001B[38;5;129;01min\u001B[39;00m trajectories[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m---> 79\u001B[0m         \u001B[43magent_landlord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;66;03m# for ts in trajectories[1]:\u001B[39;00m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;66;03m#     agent_peasant1.feed(ts)\u001B[39;00m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    100\u001B[0m \n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# Get the paths\u001B[39;00m\n\u001B[0;32m    102\u001B[0m csv_path, fig_path \u001B[38;5;241m=\u001B[39m logger\u001B[38;5;241m.\u001B[39mcsv_path, logger\u001B[38;5;241m.\u001B[39mfig_path\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:131\u001B[0m, in \u001B[0;36mDQNAgent.feed\u001B[1;34m(self, ts)\u001B[0m\n\u001B[0;32m    129\u001B[0m tmp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_t \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreplay_memory_init_size\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tmp\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m tmp\u001B[38;5;241m%\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_every \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 131\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:214\u001B[0m, in \u001B[0;36mDQNAgent.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;66;03m# Perform gradient descent update\u001B[39;00m\n\u001B[0;32m    212\u001B[0m state_batch \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(state_batch)\n\u001B[1;32m--> 214\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124mINFO - Step \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, rl-loss: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_t, loss), end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;66;03m# Update the target estimator\u001B[39;00m\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\rlcard\\agents\\dqn_agent.py:331\u001B[0m, in \u001B[0;36mEstimator.update\u001B[1;34m(self, s, a, y)\u001B[0m\n\u001B[0;32m    329\u001B[0m batch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmse_loss(Q, y)\n\u001B[0;32m    330\u001B[0m batch_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    332\u001B[0m batch_loss \u001B[38;5;241m=\u001B[39m batch_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqnet\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m--> 140\u001B[0m     out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    141\u001B[0m     obj\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 23\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\torch\\optim\\adam.py:178\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure, grad_scaler)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;129m@_use_grad_for_differentiable\u001B[39m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, closure\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, grad_scaler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;124;03m\"\"\"Performs a single optimization step.\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \n\u001B[0;32m    172\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m            supplied from ``grad_scaler.step(optimizer)``.\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 178\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_graph_capture_health_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    181\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mH:\\UniDrive\\OneDrive - UGent\\other stuff\\project\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001B[0m, in \u001B[0;36mOptimizer._cuda_graph_capture_health_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    104\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    105\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but this instance was constructed with capturable=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    108\u001B[0m     (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_warned_capturable_if_run_uncaptured\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m))\n\u001B[1;32m--> 109\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefaults\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m capturing)\n\u001B[0;32m    111\u001B[0m ):\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWarning: This instance was constructed with capturable=True, but step() \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    113\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis running without CUDA graph capture. If you never intend to graph-capture this \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    114\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minstance, capturable=True can impair performance, and you should set capturable=False.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warned_capturable_if_run_uncaptured \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'capturable'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import rlcard\n",
    "from rlcard.agents import RandomAgent\n",
    "import os\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "def train(eval):\n",
    "\n",
    "    # Check whether gpu is available\n",
    "    device = get_device()\n",
    "\n",
    "    # Seed numpy, torch, random\n",
    "    set_seed(42)\n",
    "\n",
    "    num_episodes = 500000\n",
    "    num_eval_games = 2000\n",
    "\n",
    "    # Make the environment with seed\n",
    "    env = rlcard.make(\n",
    "        \"doudizhu\",\n",
    "        config={\n",
    "            'seed': 42,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize the agent and use random agents as opponents\n",
    "    from rlcard.agents import DQNAgent\n",
    "\n",
    "    agent_peasant1 = DQNAgent(\n",
    "        num_actions=env.num_actions,\n",
    "        state_shape=env.state_shape[1],\n",
    "        mlp_layers=[64,64],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    agent_peasant2 = DQNAgent(\n",
    "        num_actions=env.num_actions,\n",
    "        state_shape=env.state_shape[2],\n",
    "        mlp_layers=[64,64],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "#     agent_landlord = torch.load(\"experiments/doudizhu_dqn/model20000.pth\")\n",
    "    agent_landlord = DQNAgent(\n",
    "        num_actions=env.num_actions,\n",
    "        state_shape=env.state_shape[0],\n",
    "        mlp_layers=[64,64],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    agents = [agent_landlord\n",
    "        , agent_peasant1, agent_peasant2\n",
    "              ]\n",
    "#     for _ in range(1, 3):\n",
    "#         agents.append(RandomAgent(num_actions=env.num_actions))\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    # Start training\n",
    "    with Logger(\"experiments/doudizhu_dqn/\") as logger:\n",
    "        for episode in range(0, num_episodes):\n",
    "            # Generate data from the environment\n",
    "            trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "            # Reorganaize the data to be state, action, reward, next_state, done\n",
    "            trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "            # Feed transitions into agent memory, and train the agent\n",
    "            # Here, we assume that DQN always plays the first position\n",
    "            # and the other players play randomly (if any)\n",
    "            for ts in trajectories[0]:\n",
    "                agent_landlord.feed(ts)\n",
    "\n",
    "            for ts in trajectories[1]:\n",
    "                agent_peasant1.feed(ts)\n",
    "\n",
    "            for ts in trajectories[2]:\n",
    "                agent_peasant2.feed(ts)\n",
    "\n",
    "            # Evaluate the performance. Play with random agents.\n",
    "            # if eval:\n",
    "            #     logger.log_performance(\n",
    "            #         env.timestep,\n",
    "            #         tournament(\n",
    "            #             env,\n",
    "            #             num_eval_games,\n",
    "            #         )[0]\n",
    "            #     )\n",
    "\n",
    "            if episode % 1000 == 0 and episode:\n",
    "                save_path = os.path.join(\"experiments/doudizhu_dqn/\", f'model_landlord_{episode}.pth')\n",
    "                torch.save(agent_landlord, save_path)\n",
    "                save_path = os.path.join(\"experiments/doudizhu_dqn/\", f'model_peasant1_{episode}.pth')\n",
    "                torch.save(agent_peasant1, save_path)\n",
    "                save_path = os.path.join(\"experiments/doudizhu_dqn/\", f'model_peasant2_{episode}.pth')\n",
    "                torch.save(agent_peasant2, save_path)\n",
    "\n",
    "        # Get the paths\n",
    "        csv_path, fig_path = logger.csv_path, logger.fig_path\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plot_curve(csv_path, fig_path, \"dqn\")\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(\"experiments/doudizhu_dqn/\", 'model.pth')\n",
    "    torch.save(agent_landlord, save_path)\n",
    "    print('Model saved in', save_path)\n",
    "train(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
